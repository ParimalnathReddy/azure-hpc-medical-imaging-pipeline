# ğŸ©º Chest X-Ray Pneumonia Detection with Azure HPC

[![Python 3.8+](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![Azure](https://img.shields.io/badge/Azure-Blob%20Storage-0089D6.svg)](https://azure.microsoft.com/)
[![SLURM](https://img.shields.io/badge/HPC-SLURM-green.svg)](https://slurm.schedmd.com/)

An end-to-end **deep learning pipeline** for automated pneumonia detection from chest X-ray images. This project demonstrates production-ready ML engineering practices including cloud data integration, HPC job scheduling, model training, evaluation, and artifact management.

---

## ğŸ“‹ Table of Contents

- [Project Overview](#-project-overview)
- [Key Features](#-key-features)
- [Architecture](#-architecture)
- [Tech Stack](#-tech-stack)
- [Project Structure](#-project-structure)
- [Quick Start](#-quick-start)
- [Configuration](#-configuration)
- [Training Pipeline](#-training-pipeline)
- [Model Performance](#-model-performance)
- [Cloud Integration](#-cloud-integration)
- [Future Improvements](#-future-improvements)

---

## ğŸ¯ Project Overview

This project implements a **binary classification model** to detect pneumonia from chest X-ray images using transfer learning with ResNet-50. The pipeline is designed for **scalable HPC environments** with seamless Azure Blob Storage integration for data management and artifact versioning.

### Problem Statement
Pneumonia is a leading cause of death worldwide. Early and accurate diagnosis through chest X-rays can significantly improve patient outcomes. This project automates the detection process using deep learning, achieving **high accuracy** on the benchmark dataset.

### Dataset
- **Source**: [Chest X-Ray Images (Pneumonia)](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia) from Kaggle
- **Classes**: NORMAL (healthy) vs PNEUMONIA (infected)
- **Total Images**: ~5,800 labeled chest X-ray images
- **Split**: Train / Validation / Test

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ”„ **End-to-End Pipeline** | Single SLURM script handles data download â†’ training â†’ evaluation â†’ cloud upload |
| â˜ï¸ **Azure Integration** | Automatic data fetching from Azure Blob Storage with SAS token authentication |
| ğŸ–¥ï¸ **HPC Ready** | Optimized for SLURM-based GPU clusters (tested on V100-32GB) |
| ğŸ“Š **Auto-Generated Reports** | Training metrics, confusion matrix, and loss curves saved automatically |
| ğŸ” **Reproducible** | Seeded experiments with YAML configuration files |
| ğŸ“¦ **Artifact Versioning** | Run outputs uploaded to Azure with timestamp-based versioning |

---

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           SLURM HPC Cluster                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚ Azure Blob   â”‚â”€â”€â”€â”€â–¶â”‚  GPU Node    â”‚â”€â”€â”€â”€â–¶â”‚  Azure Blob Storage  â”‚    â”‚
â”‚   â”‚ (Datasets)   â”‚     â”‚  - Download  â”‚     â”‚  (Artifacts/Runs)    â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  - Train     â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                        â”‚  - Evaluate  â”‚                                  â”‚
â”‚                        â”‚  - Report    â”‚                                  â”‚
â”‚                        â”‚  - Upload    â”‚                                  â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Training Flow

```mermaid
flowchart LR
    A[Azure Blob Storage] -->|azcopy| B[Local Data Dir]
    B --> C[DataLoader]
    C --> D[ResNet-50 Model]
    D --> E[Training Loop]
    E --> F[Validation]
    F --> G[Test Evaluation]
    G --> H[Generate Reports]
    H -->|azcopy| I[Azure Artifacts]
```

---

## ğŸ›  Tech Stack

| Category | Technologies |
|----------|--------------|
| **Deep Learning** | PyTorch, TorchVision, CUDA 12.6 |
| **Model Architecture** | ResNet-50 (ImageNet pretrained) |
| **Data Processing** | NumPy, Pillow, scikit-learn |
| **Visualization** | Matplotlib |
| **Configuration** | PyYAML |
| **Cloud Storage** | Azure Blob Storage, azcopy |
| **HPC** | SLURM Workload Manager |
| **Hardware** | NVIDIA Tesla V100S-32GB |

---

## ğŸ“ Project Structure

```
azure-hpc-medical-imaging/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ train_resnet50.yaml      # Hyperparameters & paths
â”œâ”€â”€ slurm/
â”‚   â””â”€â”€ end_to_end_train_upload.sbatch  # HPC job script
â”œâ”€â”€ src/
â”‚   â””â”€â”€ training/
â”‚       â”œâ”€â”€ train.py             # Main training script
â”‚       â””â”€â”€ make_report.py       # Metrics visualization
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### Output Structure (per run)
```
work/runs/YYYYMMDD_HHMMSS_resnet50/
â”œâ”€â”€ best_model.pt                # Trained model weights
â”œâ”€â”€ config.yaml                  # Run configuration
â”œâ”€â”€ metrics.json                 # All metrics (train/val/test)
â”œâ”€â”€ report.json                  # Summary report
â””â”€â”€ plots/
    â”œâ”€â”€ loss.png                 # Training loss curve
    â””â”€â”€ confusion_matrix.png     # Test set confusion matrix
```

---

## ğŸš€ Quick Start

### Prerequisites

1. **Environment Setup**
   ```bash
   conda create -n infra_env python=3.10
   conda activate infra_env
   pip install -r requirements.txt
   ```

2. **Azure SAS Tokens** (required for cloud data access)
   ```bash
   export AZ_DATASETS_SAS_URL="https://<storage>.blob.core.windows.net/datasets?<sas-token>"
   export AZ_ARTIFACTS_SAS_URL="https://<storage>.blob.core.windows.net/artifacts?<sas-token>"
   ```

### Run Training

```bash
# Submit to SLURM cluster
cd azure-hpc-medical-imaging
sbatch slurm/end_to_end_train_upload.sbatch
```

### Monitor Job
```bash
# Check job status
squeue -u $USER

# View logs (replace JOB_ID)
tail -f ../work/logs/cxr_e2e_r50_<JOB_ID>.out
```

---

## âš™ï¸ Configuration

Edit `configs/train_resnet50.yaml` to customize training:

```yaml
# Model & Training
seed: 42              # Reproducibility
epochs: 5             # Training epochs
batch_size: 32        # Batch size
lr: 0.0003            # Learning rate (AdamW)
num_workers: 4        # DataLoader workers
img_size: 224         # Input image size
model: resnet50       # Model architecture
num_classes: 2        # NORMAL vs PNEUMONIA

# Paths
paths:
  local_data_root: /path/to/data/chestxray/raw
  run_root: /path/to/work/runs
```

---

## ğŸ”¬ Training Pipeline

### 1. Data Loading
- Images loaded via `torchvision.datasets.ImageFolder`
- **Training augmentations**: Resize â†’ RandomHorizontalFlip â†’ Normalize
- **Evaluation**: Resize â†’ Normalize

### 2. Model Architecture
```
ResNet-50 (ImageNet pretrained)
â”‚
â”œâ”€â”€ Backbone: 50-layer residual network
â”‚   â””â”€â”€ Frozen: No (full fine-tuning)
â”‚
â””â”€â”€ Classifier Head: Linear(2048 â†’ 2)
```

### 3. Training Loop
- **Optimizer**: AdamW
- **Loss**: CrossEntropyLoss
- **Validation**: Per-epoch accuracy, F1, AUC-ROC

### 4. Output Artifacts
| Artifact | Description |
|----------|-------------|
| `best_model.pt` | PyTorch model state dict |
| `metrics.json` | Full training history + test metrics |
| `report.json` | Compact summary for dashboards |
| `plots/loss.png` | Training loss curve |
| `plots/confusion_matrix.png` | Test set confusion matrix |

---

## ğŸ“ˆ Model Performance

### Latest Training Run Results

| Metric | Validation (Epoch 5) | Test Set |
|--------|---------------------|----------|
| **Accuracy** | 100.0% | TBD |
| **F1 Score** | 1.0000 | TBD |
| **AUC-ROC** | 1.0000 | TBD |

### Training Progress
```
Epoch 1: loss=0.1094  val_acc=68.75%  val_f1=0.76  val_auc=1.00
Epoch 2: loss=0.0522  val_acc=68.75%  val_f1=0.76  val_auc=1.00
Epoch 3: loss=0.0366  val_acc=87.50%  val_f1=0.89  val_auc=1.00
Epoch 4: loss=0.0274  val_acc=75.00%  val_f1=0.80  val_auc=1.00
Epoch 5: loss=0.0266  val_acc=100.0%  val_f1=1.00  val_auc=1.00
```

---

## â˜ï¸ Cloud Integration

### Data Flow

1. **Download** (Azure â†’ Local)
   ```bash
   azcopy copy "${AZURE_BLOB}/chestxray/raw/?${SAS}" "./data/" --recursive
   ```

2. **Upload Artifacts** (Local â†’ Azure)
   ```bash
   azcopy copy "./runs/${RUN_ID}/*" "${AZURE_BLOB}/runs/${RUN_ID}/" --recursive
   ```

### Benefits
- ğŸ“¦ **Centralized datasets** across HPC nodes
- ğŸ”„ **Versioned artifacts** with timestamp-based directories
- ğŸ” **Secure access** via SAS tokens (no credentials in code)
- ğŸŒ **Accessible results** from anywhere via Azure portal

---

## ğŸ”® Future Improvements

- [ ] **Multi-class Classification**: Extend to detect specific pneumonia types (bacterial vs viral)
- [ ] **Model Registry**: Integration with MLflow or Azure ML for model versioning
- [ ] **Hyperparameter Tuning**: Automated search with Optuna or Ray Tune
- [ ] **Grad-CAM Visualization**: Explainability for medical professionals
- [ ] **REST API Deployment**: FastAPI + Docker for inference serving
- [ ] **CI/CD Pipeline**: Automated testing and deployment with GitHub Actions

---

## ğŸ‘¤ Author

**Pari Kodumuru**  
*ML Engineer | Medical Imaging | HPC Computing*

---

## ğŸ“„ License

This project is for educational and research purposes.

---

<p align="center">
  <i>Built with ğŸ”¬ for advancing medical AI on HPC infrastructure</i>
</p>
